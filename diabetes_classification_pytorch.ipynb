{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "diabetes_classification_pytorch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOhSKIVLu9JM"
      },
      "source": [
        "This is an example of using the neural network implemented using **Pytorch** for classifying the tabular data (diabetes_data) downloaded from [UCI repository](https://archive.ics.uci.edu/ml/datasets.php)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydIA7jp72OwV"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "if not sys.warnoptions:\n",
        "    import warnings\n",
        "    warnings.simplefilter(\"ignore\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6e8AlAirZJ9"
      },
      "source": [
        "The **preprocessing** function takes data frame or table (df) and returns the table with numerical values.The **categorical_cols** is a list which stores all the categorical features.\n",
        "The [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) is a Sklearn function, which take a categorical column and encode it to numerical value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bimZHbV-1Lt2"
      },
      "source": [
        "def preprocessing(df):\n",
        "\tcategorical_cols=[]\n",
        "\tfor c in data.columns:\n",
        "\t\tif data[c].dtype not in ['int64','int32','float64','float64']:\n",
        "\t\t\tcategorical_cols.append(c)\n",
        "\t\t\n",
        "\tfor col in categorical_cols:\n",
        "\t\tdf[col] = LabelEncoder().fit_transform(df[col])\n",
        "\tdf = df.fillna(df.mean())\n",
        "\treturn df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R80Iy0a6wpNn"
      },
      "source": [
        "The class **TableDataset** inherits the **Dataset** class of Pytorch. It takes numerical tabular data (**df**), **output column** of the table, and **mode**. The value of mode may be one of the (train, valid, test). If mode = train it will return train data, and similarly for validataion and test data. The return value of this class is a dictionary which contains **features** array and **labels** array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coPNVC1I1SoV"
      },
      "source": [
        "# Dataset class\n",
        "\n",
        "class TableDataset(Dataset):\n",
        "\t\"\"\"docstring for TableDataset\"\"\"\n",
        "\tdef __init__(self, df, output_column, mode):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# self.size = df.shape[0]\n",
        "\t\tself.mode = mode\n",
        "\t\tself.labels = df[output_column]\n",
        "\t\tself.features = df[[c for c in list(df.columns) if c!=output_column]].values\n",
        "\t\tself.train_size = int(df.shape[0]*0.7)\n",
        "\t\tself.val_size = int(df.shape[0]*0.15)\n",
        "\t\tself.test_size = int(df.shape[0]*0.15)\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\tif self.mode=='train':\n",
        "\t\t\treturn self.train_size\n",
        "\t\telif self.mode=='valid':\n",
        "\t\t\treturn self.val_size\n",
        "\t\telse: return self.test_size\n",
        "\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\tif self.mode=='valid':\n",
        "\t\t\tindex+=self.train_size\n",
        "\t\telif self.mode=='test':\n",
        "\t\t\tindex+=self.train_size+self.val_size\n",
        "\t\t# print(self.features[index])\n",
        "\t\t# print(self.labels[index])\n",
        "\t\tsample = {'features':self.features[index], \n",
        "\t\t\t\t'labels':self.labels[index]}\n",
        "\t\treturn sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs6dgMjvyTkX"
      },
      "source": [
        "Here, I have defined the **Model** class contains a two layered neural network, where the first layer contains 200 neurons, the second layer contains 100 neurons. Model takes the features and returns the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8de2nW81Y4O"
      },
      "source": [
        "# The NN Model\t\t\n",
        "\n",
        "class Model(nn.Module):\n",
        "\t\"\"\"docstring for Model\"\"\"\n",
        "\tdef __init__(self, num_features, num_classes):\n",
        "\t\tsuper().__init__()\n",
        "\t\t# print(num_features)\n",
        "\t\tself.fc1 = nn.Linear(num_features, 200)\n",
        "\t\tself.bn1 = nn.BatchNorm1d(200)\n",
        "\t\tself.fc2 = nn.Linear(200, 100)\n",
        "\t\tself.bn2 = nn.BatchNorm1d(100)\n",
        "\t\tself.fc3 = nn.Linear(100, num_classes)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\t# print(x.shape)\n",
        "\t\tx = F.relu(self.bn1(self.fc1(x)))\n",
        "\t\tx = F.relu(self.bn2(self.fc2(x)))\n",
        "\t\tx = torch.sigmoid(self.fc3(x))\n",
        "\t\treturn x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBSMtzyBzMd_"
      },
      "source": [
        "Now, we need to define the paths of input csv file, path to output the best checkpoint, and training hyperparameters such as **batch size**, **learning rate**, **number of epochs**. Further, I have defined the train, validation, and test loaders to load respective data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzprGXEj1lbI"
      },
      "source": [
        "csv_filepath = '/content/drive/MyDrive/diabetes_data_upload.csv'\n",
        "save_path = '/content/drive/MyDrive/best_checkpoint.pt'\n",
        "batch_size=10\n",
        "learning_rate=0.001\t\n",
        "n_epochs = 50\n",
        "\n",
        "data = pd.read_csv(csv_filepath)\n",
        "df = preprocessing(data)\n",
        "# print(df.shape)\n",
        "\n",
        "train_data = TableDataset(df, output_column='class', mode='train')\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "valid_data = TableDataset(df, output_column='class', mode='valid')\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size)\n",
        "\n",
        "test_data = TableDataset(df, output_column='class', mode='test')\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "print('train size = '+str(len(train_data)))\n",
        "print('valid size = '+str(len(valid_data)))\n",
        "print('test size = '+str(len(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP5_WjfAAwFC"
      },
      "source": [
        "Now, we create instance of Model class. The parameters **num_features=16** is due to 16 columns (features) of the tabular data and **num_class=1** is number of bits requred for representing two classes.\n",
        "\n",
        "**BCELoss** (Binary Cross Entropy Loss) is used which takes two 1D arrays. The first array is outputs of model and second is true labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hws8STa6_DWl"
      },
      "source": [
        "model = Model(num_features=16, num_classes=1)\n",
        "# print(model)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.BCELoss()\n",
        "best_val_acc = 0\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  epoch_train_loss = 0\n",
        "  epoch_train_acc = 0\n",
        "  epoch_valid_loss = 0\n",
        "  epoch_valid_acc = 0\n",
        "  best_epoch = 0\n",
        "  model.train()\n",
        "\n",
        "# Training the model\n",
        "\n",
        "  for i, train_batch in enumerate(train_loader):\n",
        "\n",
        "    feats = torch.tensor(train_batch['features']).float()\n",
        "    labels = torch.tensor(train_batch['labels']).float()\n",
        "\n",
        "    outputs = model(feats).view(-1)\n",
        "    loss = criterion(outputs, labels)\n",
        "    epoch_train_loss+=loss\n",
        "\n",
        "    accuracy = ((outputs>0.5).float()==labels).float().mean()\n",
        "    epoch_train_acc+=accuracy\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  for i, valid_batch in enumerate(valid_loader):\n",
        "    feats = torch.tensor(valid_batch['features']).float()\n",
        "    labels = torch.tensor(valid_batch['labels']).float()\n",
        "\n",
        "    outputs = model(feats).view(-1)\n",
        "    loss = criterion(outputs, labels)\n",
        "    epoch_valid_loss+=loss\n",
        "\n",
        "    accuracy = ((outputs>0.5).float()==labels).float().mean()\n",
        "    epoch_valid_acc+=accuracy\n",
        "\n",
        "  print(\"Ep:\",epoch,\n",
        "        \"TrainLoss: {:.4f}\" .format(epoch_train_loss.item()/len(train_loader)),\n",
        "        \"TrainAcc: {:.2f}%\" .format(100*epoch_train_acc.item()/len(train_loader)),\n",
        "        \"ValLoss: {:.4f}\" .format(epoch_valid_loss.item()/len(valid_loader)),\n",
        "        \"ValAcc: {:.2f}%\" .format(100*epoch_valid_acc.item()/len(valid_loader)))\n",
        "\n",
        "# Saving the best model checkpoint\n",
        "\n",
        "  if best_val_acc < 100*epoch_valid_acc.item()/len(valid_loader):\n",
        "    best_val_acc = 100*epoch_valid_acc.item()/len(valid_loader)\n",
        "    best_epoch = epoch\n",
        "\n",
        "    torch.save({\n",
        "          'epoch': best_epoch,\n",
        "          'model_state_dict':model.state_dict(),\n",
        "          'optimizer_state_dict':optimizer.state_dict(),\n",
        "          'train_loss': epoch_train_loss.item()/len(train_loader),\n",
        "          'valid_loss': epoch_valid_loss.item()/len(valid_loader),\n",
        "          'train_acc':100*epoch_train_acc.item()/len(train_loader),\n",
        "          'valid_acc':100*epoch_valid_acc.item()/len(valid_loader)\n",
        "      }, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOxhhWnnCddO"
      },
      "source": [
        "Now, we test the best model (which has highest validation accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gITpag8ioY1f"
      },
      "source": [
        "# Testing Phase of the Model\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "\n",
        "  checkpoint = torch.load(save_path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  test_acc = 0\n",
        "  for i, test_batch in enumerate(test_loader):\n",
        "      feats = torch.tensor(test_batch['features']).float()\n",
        "      labels = torch.tensor(test_batch['labels']).float()\n",
        "\n",
        "      outputs = model(feats).view(-1)\n",
        "\n",
        "      accuracy = ((outputs>0.5).float()==labels).float().mean()\n",
        "      test_acc+=accuracy\n",
        "  print(\"Testing best model found after epoch: {}, TrainAcc:{:.2f}%, ValAcc:{:.2f}%\".format(\n",
        "      checkpoint['epoch'], checkpoint['train_acc'], checkpoint['valid_acc']))\n",
        "  print(\"Test Accuracy: {:.2f}%\" .format(100*test_acc.item()/len(test_loader)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}